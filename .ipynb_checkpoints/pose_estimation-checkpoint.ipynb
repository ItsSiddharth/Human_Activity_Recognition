{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mk5rpIn6X7qB",
    "outputId": "7ce24257-f6fb-4f61-e239-af1ef476b765"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "NBQ59F5jZOA3",
    "outputId": "a0356eb6-e2f3-49e6-fbda-5628bd436689"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/content/training_data_yoga.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fbf9168c438e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mintermediate_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_guitar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_yoga\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/training_data_yoga.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf_guitar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/training_data_guitar.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_guitar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/content/training_data_yoga.csv' does not exist"
     ]
    }
   ],
   "source": [
    "concatinated_x_y = []\n",
    "concatinated_frames = []\n",
    "intermediate_list = []\n",
    "X_guitar = []\n",
    "df_yoga = pd.read_csv('/content/training_data_yoga.csv')\n",
    "df_guitar = pd.read_csv('/content/training_data_guitar.csv')\n",
    "print(df_guitar['x'].head(17))\n",
    "print(df_guitar['y'].head(17))\n",
    "x_cords = list(df_guitar['x'])\n",
    "y_cords = list(df_guitar['y'])\n",
    "print(len(x_cords)) # ==> 64107\n",
    "for i in range(0, len(x_cords)):\n",
    "  if i%17 ==0 and i != 0:\n",
    "    concatinated_frames.append(intermediate_list)\n",
    "    intermediate_list = []\n",
    "    intermediate_list.append(x_cords[i])\n",
    "    intermediate_list.append(y_cords[i])\n",
    "  else :\n",
    "    intermediate_list.append(x_cords[i])\n",
    "    intermediate_list.append(y_cords[i])\n",
    "print(len(concatinated_frames[0]))\n",
    "\n",
    "intermediate_list = []\n",
    "for i in range(0, len(concatinated_frames)):\n",
    "  if i%9 ==0 and i != 0:\n",
    "    X_guitar.append(intermediate_list)\n",
    "    intermediate_list = []\n",
    "    intermediate_list.append(concatinated_frames[i])\n",
    "  else :\n",
    "    intermediate_list.append(concatinated_frames[i])\n",
    "print(X_guitar[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "CMKW16wrbX4Y",
    "outputId": "90811d67-7c7d-42b6-a807-a80f636a52ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4     323\n",
      "5     309\n",
      "6     332\n",
      "7     228\n",
      "8     335\n",
      "9     309\n",
      "10    408\n",
      "11    311\n",
      "12    331\n",
      "13    311\n",
      "14    323\n",
      "15    320\n",
      "16    327\n",
      "Name: x, dtype: int64\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4     176\n",
      "5     232\n",
      "6     233\n",
      "7     243\n",
      "8     241\n",
      "9     247\n",
      "10    302\n",
      "11    294\n",
      "12    293\n",
      "13    352\n",
      "14    350\n",
      "15    416\n",
      "16    416\n",
      "Name: y, dtype: int64\n",
      "64107\n",
      "34\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416]]\n"
     ]
    }
   ],
   "source": [
    "concatinated_x_y = []\n",
    "concatinated_frames = []\n",
    "intermediate_list = []\n",
    "X_yoga = []\n",
    "df_yoga = pd.read_csv('training_data_yoga.csv')\n",
    "df_guitar = pd.read_csv('training_data_guitar.csv')\n",
    "print(df_yoga['x'].head(17))\n",
    "print(df_yoga['y'].head(17))\n",
    "x_cords = list(df_yoga['x'])\n",
    "y_cords = list(df_yoga['y'])\n",
    "print(len(x_cords)) # ==> 64107\n",
    "for i in range(0, len(x_cords)):\n",
    "  if i%17 ==0 and i != 0:\n",
    "    concatinated_frames.append(intermediate_list)\n",
    "    intermediate_list = []\n",
    "    intermediate_list.append(x_cords[i])\n",
    "    intermediate_list.append(y_cords[i])\n",
    "  else :\n",
    "    intermediate_list.append(x_cords[i])\n",
    "    intermediate_list.append(y_cords[i])\n",
    "print(len(concatinated_frames[0]))\n",
    "\n",
    "intermediate_list = []\n",
    "for i in range(0, len(concatinated_frames)):\n",
    "  if i%9 ==0 and i != 0:\n",
    "    X_yoga.append(intermediate_list)\n",
    "    intermediate_list = []\n",
    "    intermediate_list.append(concatinated_frames[i])\n",
    "  else :\n",
    "    intermediate_list.append(concatinated_frames[i])\n",
    "print(X_yoga[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ZvfNx__pFQdU",
    "outputId": "ecfc2b11-d96e-4116-d60a-2724c80698c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0], [291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0], [291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0], [291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0], [291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0], [291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0], [291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0], [291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0], [291, 136, 0, 0, 291, 130, 0, 0, 283, 125, 230, 218, 230, 209, 234, 239, 145, 155, 223, 219, 137, 182, 194, 274, 188, 276, 187, 320, 182, 319, 0, 0, 0, 0]]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "X_train = X_guitar + X_yoga\n",
    "print(X_train[417])\n",
    "guitar_label = np.zeros((len(X_guitar)))\n",
    "guitar_label = list(guitar_label)\n",
    "yoga_label = np.ones((len(X_yoga)))\n",
    "yoga_label = list(yoga_label)\n",
    "Y_train = guitar_label + yoga_label\n",
    "print(type(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gon8OpqdJ6Oq",
    "outputId": "d096fee5-f395-49ef-afea-523e927899a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 568   50  568 ...  302  336  423]\n",
      "  [ 568   50  568 ...  302  336  423]\n",
      "  [ 568   50  568 ...  302  336  423]\n",
      "  ...\n",
      "  [ 568   50  568 ...  302  336  423]\n",
      "  [ 568   50  568 ...  302  336  423]\n",
      "  [ 568   50  568 ...  302  336  423]]\n",
      "\n",
      " [[1198  256    0 ...  821 1208  816]\n",
      "  [1198  256    0 ...  821 1208  816]\n",
      "  [1198  256    0 ...  821 1208  816]\n",
      "  ...\n",
      "  [1198  256    0 ...  821 1208  816]\n",
      "  [1198  256    0 ...  821 1208  816]\n",
      "  [1198  256    0 ...  821 1208  816]]\n",
      "\n",
      " [[   0    0    0 ...  273  639  274]\n",
      "  [   0    0    0 ...  273  639  274]\n",
      "  [   0    0    0 ...  273  639  274]\n",
      "  ...\n",
      "  [   0    0    0 ...  273  639  274]\n",
      "  [   0    0    0 ...  273  639  274]\n",
      "  [   0    0    0 ...  273  639  274]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[   0    0  490 ...  808    0    0]\n",
      "  [   0    0  490 ...  808    0    0]\n",
      "  [   0    0  490 ...  808    0    0]\n",
      "  ...\n",
      "  [   0    0  490 ...  808    0    0]\n",
      "  [   0    0  490 ...  808    0    0]\n",
      "  [   0    0  490 ...  808    0    0]]\n",
      "\n",
      " [[ 942  280  944 ...  826  736  941]\n",
      "  [ 942  280  944 ...  826  736  941]\n",
      "  [ 942  280  944 ...  826  736  941]\n",
      "  ...\n",
      "  [ 942  280  944 ...  826  736  941]\n",
      "  [ 942  280  944 ...  826  736  941]\n",
      "  [ 942  280  944 ...  826  736  941]]\n",
      "\n",
      " [[   0    0    0 ...  546 1264  548]\n",
      "  [   0    0    0 ...  546 1264  548]\n",
      "  [   0    0    0 ...  546 1264  548]\n",
      "  ...\n",
      "  [   0    0    0 ...  546 1264  548]\n",
      "  [   0    0    0 ...  546 1264  548]\n",
      "  [   0    0    0 ...  546 1264  548]]] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = shuffle(np.array(X_train), np.array(Y_train))\n",
    "print(X_train, Y_train)\n",
    "\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "YC417kxbQUnJ",
    "outputId": "183336f4-6da7-4de2-91b4-59f61298e24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318, 9, 34)\n",
      "(100, 9, 34)\n",
      "(318,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "X_test = X_train[-100:]\n",
    "X_train = X_train[:-100]\n",
    "Y_test = Y_train[-100:]\n",
    "Y_train = Y_train[:-100]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_train = np.array(Y_train)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4LXe3PJFohB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0328 04:47:02.039182 140706085664576 deprecation.py:506] From /home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1:])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "DQ1y4Gm6I3_7",
    "outputId": "f0621015-d66a-46aa-b2f6-3862d21ebe9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - accuracy: 0.4973 - val_loss: 0.0000e+00 - val_accuracy: 0.5200\n",
      "Test loss: 0.0\n",
      "Test accuracy: 0.5199999809265137\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/INITIAL MODEL 10-64/assets\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32 \n",
    "NAME = f\"INITIAL MODEL {EPOCHS}-{BATCH_SIZE}\"\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "filepath = \"LSTM-{epoch:02d}-{val_acc:.3f}\"\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, Y_test),\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"models/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oTjcZX5L6v4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pose_estimation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
