{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mk5rpIn6X7qB",
    "outputId": "666b7d20-804d-43bb-8601-ff6160ca0b91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.virtualenvs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "NBQ59F5jZOA3",
    "outputId": "6d5376b0-ccb9-4e31-dc54-0dd03e8b83a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     315\n",
      "1     320\n",
      "2     305\n",
      "3     323\n",
      "4     243\n",
      "5     337\n",
      "6     222\n",
      "7     395\n",
      "8     145\n",
      "9     644\n",
      "10    161\n",
      "11    325\n",
      "12    231\n",
      "13    412\n",
      "14    241\n",
      "15    325\n",
      "16    226\n",
      "Name: x, dtype: int64\n",
      "0      66\n",
      "1      55\n",
      "2      57\n",
      "3      54\n",
      "4      50\n",
      "5     110\n",
      "6     107\n",
      "7     187\n",
      "8     171\n",
      "9     346\n",
      "10    220\n",
      "11    244\n",
      "12    243\n",
      "13    314\n",
      "14    342\n",
      "15    487\n",
      "16    491\n",
      "Name: y, dtype: int64\n",
      "64107\n",
      "34\n",
      "[[235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273]]\n"
     ]
    }
   ],
   "source": [
    "concatinated_x_y = []\n",
    "concatinated_frames = []\n",
    "intermediate_list = []\n",
    "X_guitar = []\n",
    "df_yoga = pd.read_csv('training_data_yoga.csv')\n",
    "df_guitar = pd.read_csv('training_data_guitar.csv')\n",
    "print(df_guitar['x'].head(17))\n",
    "print(df_guitar['y'].head(17))\n",
    "x_cords = list(df_guitar['x'])\n",
    "y_cords = list(df_guitar['y'])\n",
    "print(len(x_cords)) # ==> 64107\n",
    "for i in range(0, len(x_cords)):\n",
    "  if i%17 ==0 and i != 0:\n",
    "    concatinated_frames.append(intermediate_list)\n",
    "    intermediate_list = []\n",
    "    intermediate_list.append(x_cords[i])\n",
    "    intermediate_list.append(y_cords[i])\n",
    "  else :\n",
    "    intermediate_list.append(x_cords[i])\n",
    "    intermediate_list.append(y_cords[i])\n",
    "print(len(concatinated_frames[0]))\n",
    "\n",
    "intermediate_list = []\n",
    "for i in range(0, len(concatinated_frames)):\n",
    "  if i%9 ==0 and i != 0:\n",
    "    X_guitar.append(intermediate_list)\n",
    "    intermediate_list = []\n",
    "    intermediate_list.append(concatinated_frames[i])\n",
    "  else :\n",
    "    intermediate_list.append(concatinated_frames[i])\n",
    "print(X_guitar[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "CMKW16wrbX4Y",
    "outputId": "be993f07-f99d-41f7-c8b5-8dbfbdff81b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4     323\n",
      "5     309\n",
      "6     332\n",
      "7     228\n",
      "8     335\n",
      "9     309\n",
      "10    408\n",
      "11    311\n",
      "12    331\n",
      "13    311\n",
      "14    323\n",
      "15    320\n",
      "16    327\n",
      "Name: x, dtype: int64\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4     176\n",
      "5     232\n",
      "6     233\n",
      "7     243\n",
      "8     241\n",
      "9     247\n",
      "10    302\n",
      "11    294\n",
      "12    293\n",
      "13    352\n",
      "14    350\n",
      "15    416\n",
      "16    416\n",
      "Name: y, dtype: int64\n",
      "64107\n",
      "34\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416], [0, 0, 0, 0, 0, 0, 0, 0, 323, 176, 309, 232, 332, 233, 228, 243, 335, 241, 309, 247, 408, 302, 311, 294, 331, 293, 311, 352, 323, 350, 320, 416, 327, 416]]\n"
     ]
    }
   ],
   "source": [
    "concatinated_x_y = []\n",
    "concatinated_frames = []\n",
    "intermediate_list = []\n",
    "X_yoga = []\n",
    "df_yoga = pd.read_csv('training_data_yoga.csv')\n",
    "df_guitar = pd.read_csv('training_data_guitar.csv')\n",
    "print(df_yoga['x'].head(17))\n",
    "print(df_yoga['y'].head(17))\n",
    "x_cords = list(df_yoga['x'])\n",
    "y_cords = list(df_yoga['y'])\n",
    "print(len(x_cords)) # ==> 64107\n",
    "for i in range(0, len(x_cords)):\n",
    "  if i%17 ==0 and i != 0:\n",
    "    concatinated_frames.append(intermediate_list)\n",
    "    intermediate_list = []\n",
    "    intermediate_list.append(x_cords[i])\n",
    "    intermediate_list.append(y_cords[i])\n",
    "  else :\n",
    "    intermediate_list.append(x_cords[i])\n",
    "    intermediate_list.append(y_cords[i])\n",
    "print(len(concatinated_frames[0]))\n",
    "\n",
    "intermediate_list = []\n",
    "for i in range(0, len(concatinated_frames)):\n",
    "  if i%9 ==0 and i != 0:\n",
    "    X_yoga.append(intermediate_list)\n",
    "    intermediate_list = []\n",
    "    intermediate_list.append(concatinated_frames[i])\n",
    "  else :\n",
    "    intermediate_list.append(concatinated_frames[i])\n",
    "print(X_yoga[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ZvfNx__pFQdU",
    "outputId": "4cc42cd5-91cf-4d33-d775-1e78883707fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273], [235, 30, 238, 11, 231, 11, 244, 12, 200, 14, 247, 44, 194, 47, 253, 90, 192, 92, 284, 134, 194, 126, 248, 125, 226, 129, 289, 177, 235, 179, 293, 266, 234, 273]]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "X_train = X_guitar + X_yoga\n",
    "print(X_train[417])\n",
    "guitar_label = np.zeros((len(X_guitar)))\n",
    "guitar_label = list(guitar_label)\n",
    "yoga_label = np.ones((len(X_yoga)))\n",
    "yoga_label = list(yoga_label)\n",
    "Y_train = guitar_label + yoga_label\n",
    "print(type(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "gon8OpqdJ6Oq",
    "outputId": "d2f6a5eb-78f4-411c-8390-43ea93df074d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = shuffle(np.array(X_train), np.array(Y_train))\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "YC417kxbQUnJ",
    "outputId": "3696fb28-f026-4c87-8d49-876d6ec31615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736, 9, 34)\n",
      "(100, 9, 34)\n",
      "(736,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "X_test = X_train[-100:]\n",
    "X_train = X_train[:-100]\n",
    "Y_test = Y_train[-100:]\n",
    "Y_train = Y_train[:-100]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_train = np.array(Y_train)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4LXe3PJFohB"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DQ1y4Gm6I3_7",
    "outputId": "97cc6994-00e2-4307-eaeb-5a95cabf593a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 736 samples, validate on 100 samples\n",
      "Epoch 1/200\n",
      "736/736 [==============================] - 4s 5ms/sample - loss: 0.5917 - acc: 0.7174 - val_loss: 0.6367 - val_acc: 0.7300\n",
      "Epoch 2/200\n",
      "736/736 [==============================] - 1s 870us/sample - loss: 0.4137 - acc: 0.8207 - val_loss: 0.6054 - val_acc: 0.7700\n",
      "Epoch 3/200\n",
      "736/736 [==============================] - 1s 847us/sample - loss: 0.3918 - acc: 0.8220 - val_loss: 0.6137 - val_acc: 0.7400\n",
      "Epoch 4/200\n",
      "736/736 [==============================] - 1s 865us/sample - loss: 0.3214 - acc: 0.8641 - val_loss: 0.6074 - val_acc: 0.7300\n",
      "Epoch 5/200\n",
      "736/736 [==============================] - 1s 880us/sample - loss: 0.3342 - acc: 0.8682 - val_loss: 0.5800 - val_acc: 0.7900\n",
      "Epoch 6/200\n",
      "736/736 [==============================] - 1s 902us/sample - loss: 0.3035 - acc: 0.8764 - val_loss: 0.5652 - val_acc: 0.7700\n",
      "Epoch 7/200\n",
      "736/736 [==============================] - 1s 896us/sample - loss: 0.2622 - acc: 0.8954 - val_loss: 0.5295 - val_acc: 0.8100\n",
      "Epoch 8/200\n",
      "736/736 [==============================] - 1s 860us/sample - loss: 0.2769 - acc: 0.8886 - val_loss: 0.5240 - val_acc: 0.8100\n",
      "Epoch 9/200\n",
      "736/736 [==============================] - 1s 900us/sample - loss: 0.2522 - acc: 0.9035 - val_loss: 0.5595 - val_acc: 0.7000\n",
      "Epoch 10/200\n",
      "736/736 [==============================] - 1s 975us/sample - loss: 0.2797 - acc: 0.8818 - val_loss: 0.5338 - val_acc: 0.7600\n",
      "Epoch 11/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.2347 - acc: 0.9171 - val_loss: 0.5335 - val_acc: 0.7200\n",
      "Epoch 12/200\n",
      "736/736 [==============================] - 1s 940us/sample - loss: 0.2259 - acc: 0.9076 - val_loss: 0.5418 - val_acc: 0.7500\n",
      "Epoch 13/200\n",
      "736/736 [==============================] - 1s 901us/sample - loss: 0.2385 - acc: 0.9130 - val_loss: 0.4990 - val_acc: 0.8100\n",
      "Epoch 14/200\n",
      "736/736 [==============================] - 1s 937us/sample - loss: 0.2024 - acc: 0.9226 - val_loss: 0.5235 - val_acc: 0.7600\n",
      "Epoch 15/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.2095 - acc: 0.9158 - val_loss: 0.5549 - val_acc: 0.7600\n",
      "Epoch 16/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.2079 - acc: 0.9144 - val_loss: 0.6092 - val_acc: 0.7500\n",
      "Epoch 17/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.2186 - acc: 0.9198 - val_loss: 0.6362 - val_acc: 0.7600\n",
      "Epoch 18/200\n",
      "736/736 [==============================] - 1s 900us/sample - loss: 0.2297 - acc: 0.9103 - val_loss: 0.6327 - val_acc: 0.7500\n",
      "Epoch 19/200\n",
      "736/736 [==============================] - 1s 820us/sample - loss: 0.2355 - acc: 0.8995 - val_loss: 0.6885 - val_acc: 0.7500\n",
      "Epoch 20/200\n",
      "736/736 [==============================] - 1s 888us/sample - loss: 0.2215 - acc: 0.9171 - val_loss: 0.7055 - val_acc: 0.7100\n",
      "Epoch 21/200\n",
      "736/736 [==============================] - 1s 865us/sample - loss: 0.2399 - acc: 0.9049 - val_loss: 0.7493 - val_acc: 0.7700\n",
      "Epoch 22/200\n",
      "736/736 [==============================] - 1s 852us/sample - loss: 0.1829 - acc: 0.9361 - val_loss: 0.8139 - val_acc: 0.7400\n",
      "Epoch 23/200\n",
      "736/736 [==============================] - 1s 868us/sample - loss: 0.2212 - acc: 0.9062 - val_loss: 0.7094 - val_acc: 0.7500\n",
      "Epoch 24/200\n",
      "736/736 [==============================] - 1s 866us/sample - loss: 0.1961 - acc: 0.9198 - val_loss: 0.7235 - val_acc: 0.7400\n",
      "Epoch 25/200\n",
      "736/736 [==============================] - 1s 901us/sample - loss: 0.1775 - acc: 0.9198 - val_loss: 0.7157 - val_acc: 0.8000\n",
      "Epoch 26/200\n",
      "736/736 [==============================] - 1s 836us/sample - loss: 0.2665 - acc: 0.8967 - val_loss: 0.7420 - val_acc: 0.7800\n",
      "Epoch 27/200\n",
      "736/736 [==============================] - 1s 853us/sample - loss: 0.2128 - acc: 0.9226 - val_loss: 0.7758 - val_acc: 0.7400\n",
      "Epoch 28/200\n",
      "736/736 [==============================] - 1s 856us/sample - loss: 0.2146 - acc: 0.9253 - val_loss: 0.7584 - val_acc: 0.7400\n",
      "Epoch 29/200\n",
      "736/736 [==============================] - 1s 843us/sample - loss: 0.1935 - acc: 0.9253 - val_loss: 0.8272 - val_acc: 0.7600\n",
      "Epoch 30/200\n",
      "736/736 [==============================] - 1s 900us/sample - loss: 0.2208 - acc: 0.9198 - val_loss: 0.8114 - val_acc: 0.7600\n",
      "Epoch 31/200\n",
      "736/736 [==============================] - 1s 948us/sample - loss: 0.1953 - acc: 0.9198 - val_loss: 0.7370 - val_acc: 0.7900\n",
      "Epoch 32/200\n",
      "736/736 [==============================] - 1s 897us/sample - loss: 0.1999 - acc: 0.9212 - val_loss: 0.8040 - val_acc: 0.7400\n",
      "Epoch 33/200\n",
      "736/736 [==============================] - 1s 943us/sample - loss: 0.2328 - acc: 0.9103 - val_loss: 0.8194 - val_acc: 0.8100\n",
      "Epoch 34/200\n",
      "736/736 [==============================] - 1s 884us/sample - loss: 0.2383 - acc: 0.9185 - val_loss: 0.7876 - val_acc: 0.7800\n",
      "Epoch 35/200\n",
      "736/736 [==============================] - 1s 953us/sample - loss: 0.2002 - acc: 0.9361 - val_loss: 0.8435 - val_acc: 0.7200\n",
      "Epoch 36/200\n",
      "736/736 [==============================] - 1s 941us/sample - loss: 0.1953 - acc: 0.9239 - val_loss: 0.8005 - val_acc: 0.7400\n",
      "Epoch 37/200\n",
      "736/736 [==============================] - 1s 925us/sample - loss: 0.2138 - acc: 0.9117 - val_loss: 0.9186 - val_acc: 0.7500\n",
      "Epoch 38/200\n",
      "736/736 [==============================] - 1s 910us/sample - loss: 0.1978 - acc: 0.9266 - val_loss: 0.8786 - val_acc: 0.7800\n",
      "Epoch 39/200\n",
      "736/736 [==============================] - 1s 901us/sample - loss: 0.1947 - acc: 0.9117 - val_loss: 0.9042 - val_acc: 0.7600\n",
      "Epoch 40/200\n",
      "736/736 [==============================] - 1s 895us/sample - loss: 0.1996 - acc: 0.9090 - val_loss: 0.8334 - val_acc: 0.7600\n",
      "Epoch 41/200\n",
      "736/736 [==============================] - 1s 910us/sample - loss: 0.1794 - acc: 0.9348 - val_loss: 0.7634 - val_acc: 0.8000\n",
      "Epoch 42/200\n",
      "736/736 [==============================] - 1s 890us/sample - loss: 0.2183 - acc: 0.9022 - val_loss: 0.7946 - val_acc: 0.7800\n",
      "Epoch 43/200\n",
      "736/736 [==============================] - 1s 908us/sample - loss: 0.1997 - acc: 0.9239 - val_loss: 0.7737 - val_acc: 0.7800\n",
      "Epoch 44/200\n",
      "736/736 [==============================] - 1s 909us/sample - loss: 0.1676 - acc: 0.9361 - val_loss: 0.8456 - val_acc: 0.7700\n",
      "Epoch 45/200\n",
      "736/736 [==============================] - 1s 866us/sample - loss: 0.1808 - acc: 0.9198 - val_loss: 0.8422 - val_acc: 0.7600\n",
      "Epoch 46/200\n",
      "736/736 [==============================] - 1s 903us/sample - loss: 0.2021 - acc: 0.9321 - val_loss: 0.8271 - val_acc: 0.7900\n",
      "Epoch 47/200\n",
      "736/736 [==============================] - 1s 892us/sample - loss: 0.1936 - acc: 0.9226 - val_loss: 1.0090 - val_acc: 0.7400\n",
      "Epoch 48/200\n",
      "736/736 [==============================] - 1s 884us/sample - loss: 0.2107 - acc: 0.9171 - val_loss: 0.9195 - val_acc: 0.8000\n",
      "Epoch 49/200\n",
      "736/736 [==============================] - 1s 880us/sample - loss: 0.2122 - acc: 0.9198 - val_loss: 0.8084 - val_acc: 0.7900\n",
      "Epoch 50/200\n",
      "736/736 [==============================] - 1s 908us/sample - loss: 0.1972 - acc: 0.9226 - val_loss: 0.7585 - val_acc: 0.7900\n",
      "Epoch 51/200\n",
      "736/736 [==============================] - 1s 904us/sample - loss: 0.1992 - acc: 0.9212 - val_loss: 0.7103 - val_acc: 0.8000\n",
      "Epoch 52/200\n",
      "736/736 [==============================] - 1s 880us/sample - loss: 0.1640 - acc: 0.9375 - val_loss: 0.9135 - val_acc: 0.7400\n",
      "Epoch 53/200\n",
      "736/736 [==============================] - 1s 898us/sample - loss: 0.1881 - acc: 0.9253 - val_loss: 0.7689 - val_acc: 0.8100\n",
      "Epoch 54/200\n",
      "736/736 [==============================] - 1s 907us/sample - loss: 0.1862 - acc: 0.9361 - val_loss: 0.8578 - val_acc: 0.7700\n",
      "Epoch 55/200\n",
      "736/736 [==============================] - 1s 910us/sample - loss: 0.1706 - acc: 0.9416 - val_loss: 0.8749 - val_acc: 0.8000\n",
      "Epoch 56/200\n",
      "736/736 [==============================] - 1s 904us/sample - loss: 0.1521 - acc: 0.9443 - val_loss: 0.8198 - val_acc: 0.8200\n",
      "Epoch 57/200\n",
      "736/736 [==============================] - 1s 906us/sample - loss: 0.1713 - acc: 0.9321 - val_loss: 0.9219 - val_acc: 0.7300\n",
      "Epoch 58/200\n",
      "736/736 [==============================] - 1s 894us/sample - loss: 0.1685 - acc: 0.9348 - val_loss: 0.8404 - val_acc: 0.7600\n",
      "Epoch 59/200\n",
      "736/736 [==============================] - 1s 907us/sample - loss: 0.1630 - acc: 0.9457 - val_loss: 0.9625 - val_acc: 0.7500\n",
      "Epoch 60/200\n",
      "736/736 [==============================] - 1s 832us/sample - loss: 0.1853 - acc: 0.9266 - val_loss: 0.8855 - val_acc: 0.7600\n",
      "Epoch 61/200\n",
      "736/736 [==============================] - 1s 794us/sample - loss: 0.2354 - acc: 0.9062 - val_loss: 0.9125 - val_acc: 0.8000\n",
      "Epoch 62/200\n",
      "736/736 [==============================] - 1s 791us/sample - loss: 0.1667 - acc: 0.9307 - val_loss: 0.9776 - val_acc: 0.7900\n",
      "Epoch 63/200\n",
      "736/736 [==============================] - 1s 809us/sample - loss: 0.1743 - acc: 0.9321 - val_loss: 1.0795 - val_acc: 0.7500\n",
      "Epoch 64/200\n",
      "736/736 [==============================] - 1s 806us/sample - loss: 0.1820 - acc: 0.9280 - val_loss: 0.9652 - val_acc: 0.7500\n",
      "Epoch 65/200\n",
      "736/736 [==============================] - 1s 815us/sample - loss: 0.1880 - acc: 0.9280 - val_loss: 0.9323 - val_acc: 0.7600\n",
      "Epoch 66/200\n",
      "736/736 [==============================] - 1s 802us/sample - loss: 0.1761 - acc: 0.9361 - val_loss: 0.9091 - val_acc: 0.7800\n",
      "Epoch 67/200\n",
      "736/736 [==============================] - 1s 805us/sample - loss: 0.1657 - acc: 0.9307 - val_loss: 0.9879 - val_acc: 0.7600\n",
      "Epoch 68/200\n",
      "736/736 [==============================] - 1s 812us/sample - loss: 0.1619 - acc: 0.9375 - val_loss: 1.0428 - val_acc: 0.7700\n",
      "Epoch 69/200\n",
      "736/736 [==============================] - 1s 801us/sample - loss: 0.1776 - acc: 0.9293 - val_loss: 0.9407 - val_acc: 0.7400\n",
      "Epoch 70/200\n",
      "736/736 [==============================] - 1s 782us/sample - loss: 0.1576 - acc: 0.9416 - val_loss: 1.0284 - val_acc: 0.7600\n",
      "Epoch 71/200\n",
      "736/736 [==============================] - 1s 815us/sample - loss: 0.1630 - acc: 0.9307 - val_loss: 1.0879 - val_acc: 0.7200\n",
      "Epoch 72/200\n",
      "736/736 [==============================] - 1s 806us/sample - loss: 0.1937 - acc: 0.9171 - val_loss: 0.9644 - val_acc: 0.7700\n",
      "Epoch 73/200\n",
      "736/736 [==============================] - 1s 825us/sample - loss: 0.1845 - acc: 0.9375 - val_loss: 1.0082 - val_acc: 0.7700\n",
      "Epoch 74/200\n",
      "736/736 [==============================] - 1s 883us/sample - loss: 0.1666 - acc: 0.9307 - val_loss: 1.0519 - val_acc: 0.7700\n",
      "Epoch 75/200\n",
      "736/736 [==============================] - 1s 867us/sample - loss: 0.1865 - acc: 0.9266 - val_loss: 1.0386 - val_acc: 0.7500\n",
      "Epoch 76/200\n",
      "736/736 [==============================] - 1s 821us/sample - loss: 0.1649 - acc: 0.9361 - val_loss: 0.9872 - val_acc: 0.7700\n",
      "Epoch 77/200\n",
      "736/736 [==============================] - 1s 846us/sample - loss: 0.1542 - acc: 0.9321 - val_loss: 1.0634 - val_acc: 0.7700\n",
      "Epoch 78/200\n",
      "736/736 [==============================] - 1s 826us/sample - loss: 0.1667 - acc: 0.9402 - val_loss: 0.9253 - val_acc: 0.7800\n",
      "Epoch 79/200\n",
      "736/736 [==============================] - 1s 807us/sample - loss: 0.1995 - acc: 0.9293 - val_loss: 0.8670 - val_acc: 0.8100\n",
      "Epoch 80/200\n",
      "736/736 [==============================] - 1s 813us/sample - loss: 0.1397 - acc: 0.9402 - val_loss: 0.9739 - val_acc: 0.7500\n",
      "Epoch 81/200\n",
      "736/736 [==============================] - 1s 798us/sample - loss: 0.1872 - acc: 0.9307 - val_loss: 0.8811 - val_acc: 0.7400\n",
      "Epoch 82/200\n",
      "736/736 [==============================] - 1s 798us/sample - loss: 0.1650 - acc: 0.9443 - val_loss: 1.0684 - val_acc: 0.7400\n",
      "Epoch 83/200\n",
      "736/736 [==============================] - 1s 809us/sample - loss: 0.1591 - acc: 0.9402 - val_loss: 1.0406 - val_acc: 0.7100\n",
      "Epoch 84/200\n",
      "736/736 [==============================] - 1s 916us/sample - loss: 0.2020 - acc: 0.9239 - val_loss: 0.9154 - val_acc: 0.7600\n",
      "Epoch 85/200\n",
      "736/736 [==============================] - 1s 898us/sample - loss: 0.1811 - acc: 0.9280 - val_loss: 1.0234 - val_acc: 0.6700\n",
      "Epoch 86/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.1740 - acc: 0.9226 - val_loss: 0.9788 - val_acc: 0.7400\n",
      "Epoch 87/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.2024 - acc: 0.9280 - val_loss: 1.1296 - val_acc: 0.7200\n",
      "Epoch 88/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.2185 - acc: 0.9212 - val_loss: 1.0130 - val_acc: 0.7500\n",
      "Epoch 89/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.1688 - acc: 0.9497 - val_loss: 0.8612 - val_acc: 0.7800\n",
      "Epoch 90/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.1488 - acc: 0.9402 - val_loss: 1.0063 - val_acc: 0.7500\n",
      "Epoch 91/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.1651 - acc: 0.9334 - val_loss: 1.0244 - val_acc: 0.7300\n",
      "Epoch 92/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.1776 - acc: 0.9361 - val_loss: 0.9765 - val_acc: 0.7700\n",
      "Epoch 93/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.2045 - acc: 0.9253 - val_loss: 0.9989 - val_acc: 0.7200\n",
      "Epoch 94/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.1834 - acc: 0.9280 - val_loss: 0.9362 - val_acc: 0.7800\n",
      "Epoch 95/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.2049 - acc: 0.9253 - val_loss: 0.9693 - val_acc: 0.7800\n",
      "Epoch 96/200\n",
      "736/736 [==============================] - 1s 804us/sample - loss: 0.1985 - acc: 0.9334 - val_loss: 0.7999 - val_acc: 0.7700\n",
      "Epoch 97/200\n",
      "736/736 [==============================] - 1s 811us/sample - loss: 0.2194 - acc: 0.9185 - val_loss: 0.8918 - val_acc: 0.7700\n",
      "Epoch 98/200\n",
      "736/736 [==============================] - 1s 842us/sample - loss: 0.1392 - acc: 0.9429 - val_loss: 0.9525 - val_acc: 0.7900\n",
      "Epoch 99/200\n",
      "736/736 [==============================] - 1s 825us/sample - loss: 0.1911 - acc: 0.9280 - val_loss: 0.8861 - val_acc: 0.7700\n",
      "Epoch 100/200\n",
      "736/736 [==============================] - 1s 839us/sample - loss: 0.1729 - acc: 0.9280 - val_loss: 0.9275 - val_acc: 0.7700\n",
      "Epoch 101/200\n",
      "736/736 [==============================] - 1s 941us/sample - loss: 0.1876 - acc: 0.9293 - val_loss: 0.9084 - val_acc: 0.8000\n",
      "Epoch 102/200\n",
      "736/736 [==============================] - 1s 834us/sample - loss: 0.1871 - acc: 0.9198 - val_loss: 1.0192 - val_acc: 0.7500\n",
      "Epoch 103/200\n",
      "736/736 [==============================] - 1s 826us/sample - loss: 0.1590 - acc: 0.9334 - val_loss: 1.0522 - val_acc: 0.7200\n",
      "Epoch 104/200\n",
      "736/736 [==============================] - 1s 825us/sample - loss: 0.1884 - acc: 0.9226 - val_loss: 0.9692 - val_acc: 0.7500\n",
      "Epoch 105/200\n",
      "736/736 [==============================] - 1s 842us/sample - loss: 0.2042 - acc: 0.9266 - val_loss: 0.9215 - val_acc: 0.7300\n",
      "Epoch 106/200\n",
      "736/736 [==============================] - 1s 834us/sample - loss: 0.1933 - acc: 0.9253 - val_loss: 1.0057 - val_acc: 0.7200\n",
      "Epoch 107/200\n",
      "736/736 [==============================] - 1s 853us/sample - loss: 0.1750 - acc: 0.9212 - val_loss: 0.9217 - val_acc: 0.7800\n",
      "Epoch 108/200\n",
      "736/736 [==============================] - 1s 841us/sample - loss: 0.1504 - acc: 0.9402 - val_loss: 0.9855 - val_acc: 0.7600\n",
      "Epoch 109/200\n",
      "736/736 [==============================] - 1s 836us/sample - loss: 0.1645 - acc: 0.9334 - val_loss: 1.0206 - val_acc: 0.8300\n",
      "Epoch 110/200\n",
      "736/736 [==============================] - 1s 874us/sample - loss: 0.1772 - acc: 0.9321 - val_loss: 0.9694 - val_acc: 0.7900\n",
      "Epoch 111/200\n",
      "736/736 [==============================] - 1s 851us/sample - loss: 0.1772 - acc: 0.9307 - val_loss: 0.9457 - val_acc: 0.7800\n",
      "Epoch 112/200\n",
      "736/736 [==============================] - 1s 828us/sample - loss: 0.1848 - acc: 0.9307 - val_loss: 0.9011 - val_acc: 0.7900\n",
      "Epoch 113/200\n",
      "736/736 [==============================] - 1s 863us/sample - loss: 0.1996 - acc: 0.9185 - val_loss: 0.8196 - val_acc: 0.8000\n",
      "Epoch 114/200\n",
      "736/736 [==============================] - 1s 846us/sample - loss: 0.1722 - acc: 0.9375 - val_loss: 0.8150 - val_acc: 0.8000\n",
      "Epoch 115/200\n",
      "736/736 [==============================] - 1s 803us/sample - loss: 0.1768 - acc: 0.9280 - val_loss: 0.8349 - val_acc: 0.7700\n",
      "Epoch 116/200\n",
      "736/736 [==============================] - 1s 825us/sample - loss: 0.1717 - acc: 0.9321 - val_loss: 0.9210 - val_acc: 0.7900\n",
      "Epoch 117/200\n",
      "736/736 [==============================] - 1s 793us/sample - loss: 0.1896 - acc: 0.9321 - val_loss: 0.8592 - val_acc: 0.7900\n",
      "Epoch 118/200\n",
      "736/736 [==============================] - 1s 879us/sample - loss: 0.2218 - acc: 0.9212 - val_loss: 0.9576 - val_acc: 0.7200\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 1s 912us/sample - loss: 0.2137 - acc: 0.9212 - val_loss: 0.7875 - val_acc: 0.7200\n",
      "Epoch 120/200\n",
      "736/736 [==============================] - 1s 923us/sample - loss: 0.1782 - acc: 0.9321 - val_loss: 0.8173 - val_acc: 0.7600\n",
      "Epoch 121/200\n",
      "736/736 [==============================] - 1s 838us/sample - loss: 0.1755 - acc: 0.9361 - val_loss: 0.8734 - val_acc: 0.7600\n",
      "Epoch 122/200\n",
      "736/736 [==============================] - 1s 822us/sample - loss: 0.1430 - acc: 0.9443 - val_loss: 0.8279 - val_acc: 0.8200\n",
      "Epoch 123/200\n",
      "736/736 [==============================] - 1s 975us/sample - loss: 0.1810 - acc: 0.9334 - val_loss: 0.8939 - val_acc: 0.7800\n",
      "Epoch 124/200\n",
      "736/736 [==============================] - 1s 876us/sample - loss: 0.1746 - acc: 0.9321 - val_loss: 0.9553 - val_acc: 0.7600\n",
      "Epoch 125/200\n",
      "736/736 [==============================] - 1s 865us/sample - loss: 0.1686 - acc: 0.9226 - val_loss: 0.8457 - val_acc: 0.7900\n",
      "Epoch 126/200\n",
      "736/736 [==============================] - 1s 806us/sample - loss: 0.1793 - acc: 0.9266 - val_loss: 0.8805 - val_acc: 0.7900\n",
      "Epoch 127/200\n",
      "736/736 [==============================] - 1s 811us/sample - loss: 0.1633 - acc: 0.9457 - val_loss: 0.8882 - val_acc: 0.7700\n",
      "Epoch 128/200\n",
      "736/736 [==============================] - 1s 850us/sample - loss: 0.1903 - acc: 0.9212 - val_loss: 0.9377 - val_acc: 0.7700\n",
      "Epoch 129/200\n",
      "736/736 [==============================] - 1s 836us/sample - loss: 0.1657 - acc: 0.9416 - val_loss: 0.8809 - val_acc: 0.7700\n",
      "Epoch 130/200\n",
      "736/736 [==============================] - 1s 840us/sample - loss: 0.1863 - acc: 0.9375 - val_loss: 0.8142 - val_acc: 0.7900\n",
      "Epoch 131/200\n",
      "736/736 [==============================] - 1s 837us/sample - loss: 0.1564 - acc: 0.9293 - val_loss: 0.7760 - val_acc: 0.8200\n",
      "Epoch 132/200\n",
      "736/736 [==============================] - 1s 831us/sample - loss: 0.1900 - acc: 0.9293 - val_loss: 0.8577 - val_acc: 0.8300\n",
      "Epoch 133/200\n",
      "736/736 [==============================] - 1s 857us/sample - loss: 0.1591 - acc: 0.9389 - val_loss: 0.8798 - val_acc: 0.7800\n",
      "Epoch 134/200\n",
      "736/736 [==============================] - 1s 843us/sample - loss: 0.1546 - acc: 0.9470 - val_loss: 0.9240 - val_acc: 0.7500\n",
      "Epoch 135/200\n",
      "736/736 [==============================] - 1s 880us/sample - loss: 0.1991 - acc: 0.9198 - val_loss: 0.8962 - val_acc: 0.7600\n",
      "Epoch 136/200\n",
      "736/736 [==============================] - 1s 853us/sample - loss: 0.1986 - acc: 0.9144 - val_loss: 0.9660 - val_acc: 0.8000\n",
      "Epoch 137/200\n",
      "736/736 [==============================] - 1s 848us/sample - loss: 0.1919 - acc: 0.9253 - val_loss: 0.8704 - val_acc: 0.8000\n",
      "Epoch 138/200\n",
      "736/736 [==============================] - 1s 856us/sample - loss: 0.1870 - acc: 0.9293 - val_loss: 0.8969 - val_acc: 0.8000\n",
      "Epoch 139/200\n",
      "736/736 [==============================] - 1s 842us/sample - loss: 0.2010 - acc: 0.9198 - val_loss: 0.9521 - val_acc: 0.8000\n",
      "Epoch 140/200\n",
      "736/736 [==============================] - 1s 855us/sample - loss: 0.1885 - acc: 0.9226 - val_loss: 0.8352 - val_acc: 0.8100\n",
      "Epoch 141/200\n",
      "736/736 [==============================] - 1s 975us/sample - loss: 0.1806 - acc: 0.9280 - val_loss: 0.8912 - val_acc: 0.8000\n",
      "Epoch 142/200\n",
      "736/736 [==============================] - 1s 915us/sample - loss: 0.1636 - acc: 0.9416 - val_loss: 0.9540 - val_acc: 0.7700\n",
      "Epoch 143/200\n",
      "736/736 [==============================] - 1s 973us/sample - loss: 0.1754 - acc: 0.9253 - val_loss: 1.0886 - val_acc: 0.7600\n",
      "Epoch 144/200\n",
      "736/736 [==============================] - 1s 957us/sample - loss: 0.2013 - acc: 0.9293 - val_loss: 0.9350 - val_acc: 0.7900\n",
      "Epoch 145/200\n",
      "736/736 [==============================] - 1s 979us/sample - loss: 0.1937 - acc: 0.9226 - val_loss: 0.9949 - val_acc: 0.7600\n",
      "Epoch 146/200\n",
      "736/736 [==============================] - 1s 1ms/sample - loss: 0.1922 - acc: 0.9293 - val_loss: 0.9654 - val_acc: 0.7800\n",
      "Epoch 147/200\n",
      "736/736 [==============================] - 1s 882us/sample - loss: 0.1655 - acc: 0.9293 - val_loss: 0.8699 - val_acc: 0.8100\n",
      "Epoch 148/200\n",
      "736/736 [==============================] - 1s 815us/sample - loss: 0.1724 - acc: 0.9361 - val_loss: 1.0164 - val_acc: 0.8100\n",
      "Epoch 149/200\n",
      "736/736 [==============================] - 1s 819us/sample - loss: 0.1764 - acc: 0.9334 - val_loss: 1.0621 - val_acc: 0.8100\n",
      "Epoch 150/200\n",
      "736/736 [==============================] - 1s 858us/sample - loss: 0.1877 - acc: 0.9226 - val_loss: 1.0162 - val_acc: 0.7900\n",
      "Epoch 151/200\n",
      "736/736 [==============================] - 1s 816us/sample - loss: 0.1817 - acc: 0.9266 - val_loss: 1.0788 - val_acc: 0.7000\n",
      "Epoch 152/200\n",
      "736/736 [==============================] - 1s 812us/sample - loss: 0.1594 - acc: 0.9280 - val_loss: 1.0523 - val_acc: 0.7400\n",
      "Epoch 153/200\n",
      "736/736 [==============================] - 1s 838us/sample - loss: 0.2276 - acc: 0.9090 - val_loss: 1.0232 - val_acc: 0.7600\n",
      "Epoch 154/200\n",
      "736/736 [==============================] - 1s 818us/sample - loss: 0.1734 - acc: 0.9266 - val_loss: 0.8442 - val_acc: 0.8000\n",
      "Epoch 155/200\n",
      "736/736 [==============================] - 1s 832us/sample - loss: 0.1883 - acc: 0.9198 - val_loss: 0.8754 - val_acc: 0.7700\n",
      "Epoch 156/200\n",
      "736/736 [==============================] - 1s 809us/sample - loss: 0.1659 - acc: 0.9416 - val_loss: 0.7248 - val_acc: 0.8200\n",
      "Epoch 157/200\n",
      "736/736 [==============================] - 1s 844us/sample - loss: 0.1579 - acc: 0.9484 - val_loss: 0.8485 - val_acc: 0.7900\n",
      "Epoch 158/200\n",
      "736/736 [==============================] - 1s 822us/sample - loss: 0.1746 - acc: 0.9402 - val_loss: 0.8543 - val_acc: 0.7800\n",
      "Epoch 159/200\n",
      "736/736 [==============================] - 1s 811us/sample - loss: 0.1743 - acc: 0.9334 - val_loss: 0.8091 - val_acc: 0.7800\n",
      "Epoch 160/200\n",
      "736/736 [==============================] - 1s 841us/sample - loss: 0.1732 - acc: 0.9321 - val_loss: 0.9009 - val_acc: 0.7600\n",
      "Epoch 161/200\n",
      "736/736 [==============================] - 1s 855us/sample - loss: 0.1825 - acc: 0.9293 - val_loss: 0.9642 - val_acc: 0.7400\n",
      "Epoch 162/200\n",
      "736/736 [==============================] - 1s 842us/sample - loss: 0.1373 - acc: 0.9497 - val_loss: 0.9814 - val_acc: 0.7800\n",
      "Epoch 163/200\n",
      "736/736 [==============================] - 1s 833us/sample - loss: 0.1424 - acc: 0.9348 - val_loss: 1.0551 - val_acc: 0.7500\n",
      "Epoch 164/200\n",
      "736/736 [==============================] - 1s 838us/sample - loss: 0.1592 - acc: 0.9443 - val_loss: 0.9975 - val_acc: 0.7700\n",
      "Epoch 165/200\n",
      "736/736 [==============================] - 1s 924us/sample - loss: 0.1541 - acc: 0.9348 - val_loss: 1.1856 - val_acc: 0.7100\n",
      "Epoch 166/200\n",
      "736/736 [==============================] - 1s 792us/sample - loss: 0.2017 - acc: 0.9280 - val_loss: 1.0413 - val_acc: 0.7200\n",
      "Epoch 167/200\n",
      "736/736 [==============================] - 1s 781us/sample - loss: 0.2111 - acc: 0.9239 - val_loss: 0.9536 - val_acc: 0.7400\n",
      "Epoch 168/200\n",
      "736/736 [==============================] - 1s 801us/sample - loss: 0.2025 - acc: 0.9158 - val_loss: 0.9337 - val_acc: 0.7800\n",
      "Epoch 169/200\n",
      "736/736 [==============================] - 1s 938us/sample - loss: 0.1763 - acc: 0.9321 - val_loss: 1.0175 - val_acc: 0.7300\n",
      "Epoch 170/200\n",
      "736/736 [==============================] - 1s 971us/sample - loss: 0.1948 - acc: 0.9198 - val_loss: 0.9095 - val_acc: 0.7300\n",
      "Epoch 171/200\n",
      "736/736 [==============================] - 1s 934us/sample - loss: 0.1289 - acc: 0.9484 - val_loss: 0.9121 - val_acc: 0.7600\n",
      "Epoch 172/200\n",
      "736/736 [==============================] - 1s 911us/sample - loss: 0.1532 - acc: 0.9457 - val_loss: 1.0036 - val_acc: 0.7400\n",
      "Epoch 173/200\n",
      "736/736 [==============================] - 1s 928us/sample - loss: 0.1470 - acc: 0.9484 - val_loss: 0.9499 - val_acc: 0.7800\n",
      "Epoch 174/200\n",
      "736/736 [==============================] - 1s 860us/sample - loss: 0.1790 - acc: 0.9321 - val_loss: 0.8736 - val_acc: 0.8200\n",
      "Epoch 175/200\n",
      "736/736 [==============================] - 1s 956us/sample - loss: 0.1663 - acc: 0.9334 - val_loss: 0.8763 - val_acc: 0.8000\n",
      "Epoch 176/200\n",
      "736/736 [==============================] - 1s 932us/sample - loss: 0.1704 - acc: 0.9402 - val_loss: 0.8206 - val_acc: 0.8400\n",
      "Epoch 177/200\n",
      "736/736 [==============================] - 1s 844us/sample - loss: 0.1330 - acc: 0.9565 - val_loss: 1.0239 - val_acc: 0.7700\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 1s 986us/sample - loss: 0.1651 - acc: 0.9348 - val_loss: 0.8918 - val_acc: 0.7900\n",
      "Epoch 179/200\n",
      "736/736 [==============================] - 1s 836us/sample - loss: 0.1495 - acc: 0.9511 - val_loss: 0.8498 - val_acc: 0.7700\n",
      "Epoch 180/200\n",
      "736/736 [==============================] - 1s 822us/sample - loss: 0.1296 - acc: 0.9565 - val_loss: 0.9832 - val_acc: 0.7700\n",
      "Epoch 181/200\n",
      "736/736 [==============================] - 1s 805us/sample - loss: 0.1453 - acc: 0.9470 - val_loss: 1.0432 - val_acc: 0.7600\n",
      "Epoch 182/200\n",
      "736/736 [==============================] - 1s 817us/sample - loss: 0.1696 - acc: 0.9293 - val_loss: 0.9551 - val_acc: 0.7800\n",
      "Epoch 183/200\n",
      "736/736 [==============================] - 1s 809us/sample - loss: 0.1682 - acc: 0.9429 - val_loss: 0.9762 - val_acc: 0.7800\n",
      "Epoch 184/200\n",
      "736/736 [==============================] - 1s 801us/sample - loss: 0.1589 - acc: 0.9361 - val_loss: 1.0298 - val_acc: 0.8100\n",
      "Epoch 185/200\n",
      "736/736 [==============================] - 1s 795us/sample - loss: 0.1843 - acc: 0.9293 - val_loss: 0.9906 - val_acc: 0.8000\n",
      "Epoch 186/200\n",
      "736/736 [==============================] - 1s 798us/sample - loss: 0.1654 - acc: 0.9293 - val_loss: 0.9786 - val_acc: 0.8300\n",
      "Epoch 187/200\n",
      "736/736 [==============================] - 1s 785us/sample - loss: 0.1575 - acc: 0.9375 - val_loss: 1.0544 - val_acc: 0.7900\n",
      "Epoch 188/200\n",
      "736/736 [==============================] - 1s 813us/sample - loss: 0.1985 - acc: 0.9198 - val_loss: 0.9688 - val_acc: 0.8300\n",
      "Epoch 189/200\n",
      "736/736 [==============================] - 1s 799us/sample - loss: 0.1519 - acc: 0.9416 - val_loss: 0.9402 - val_acc: 0.8400\n",
      "Epoch 190/200\n",
      "736/736 [==============================] - 1s 807us/sample - loss: 0.1569 - acc: 0.9293 - val_loss: 0.9106 - val_acc: 0.8500\n",
      "Epoch 191/200\n",
      "736/736 [==============================] - 1s 813us/sample - loss: 0.1473 - acc: 0.9389 - val_loss: 1.0179 - val_acc: 0.8000\n",
      "Epoch 192/200\n",
      "736/736 [==============================] - 1s 839us/sample - loss: 0.1557 - acc: 0.9361 - val_loss: 1.0635 - val_acc: 0.7700\n",
      "Epoch 193/200\n",
      "736/736 [==============================] - 1s 845us/sample - loss: 0.1999 - acc: 0.9239 - val_loss: 0.9758 - val_acc: 0.7900\n",
      "Epoch 194/200\n",
      "736/736 [==============================] - 1s 847us/sample - loss: 0.2149 - acc: 0.9130 - val_loss: 0.8720 - val_acc: 0.8100\n",
      "Epoch 195/200\n",
      "736/736 [==============================] - 1s 824us/sample - loss: 0.1790 - acc: 0.9280 - val_loss: 0.8277 - val_acc: 0.8000\n",
      "Epoch 196/200\n",
      "736/736 [==============================] - 1s 858us/sample - loss: 0.1679 - acc: 0.9307 - val_loss: 0.9038 - val_acc: 0.7900\n",
      "Epoch 197/200\n",
      "736/736 [==============================] - 1s 850us/sample - loss: 0.1733 - acc: 0.9429 - val_loss: 0.8150 - val_acc: 0.8200\n",
      "Epoch 198/200\n",
      "736/736 [==============================] - 1s 862us/sample - loss: 0.1799 - acc: 0.9321 - val_loss: 0.7716 - val_acc: 0.8200\n",
      "Epoch 199/200\n",
      "736/736 [==============================] - 1s 837us/sample - loss: 0.1309 - acc: 0.9443 - val_loss: 0.8319 - val_acc: 0.8300\n",
      "Epoch 200/200\n",
      "736/736 [==============================] - 1s 853us/sample - loss: 0.1590 - acc: 0.9457 - val_loss: 0.8439 - val_acc: 0.8400\n",
      "Test loss: 0.8439495515823364\n",
      "Test accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64 \n",
    "NAME = f\"INITIAL MODEL {EPOCHS}-{BATCH_SIZE}\"\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "filepath = \"LSTM-{epoch:02d}-{val_acc:.3f}\"\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, Y_test),\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"{}.model\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oTjcZX5L6v4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pose_estimation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
